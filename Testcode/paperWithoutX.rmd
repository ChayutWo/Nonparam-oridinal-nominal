
---
title: "Dirichlet process mixture of products of multinomial distributions model (DPMPM)"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: spacelab
---

```{r setup, echo =FALSE, include=FALSE}
# DO NOT ALTER CODE IN THIS CHUNK
# The very first time you use this R markdown file, you should install each of the packages below.
# The same goes for other packages you might decide to use
# Remember that you only need to install each R package once in R (ever -- unless you change computers).
# All you need to do whenever you need to use the package again (after restarting the R session),
# is to use the library function to call the package.
# For example, type install.packages("knitr") in the console to install the knitr package. 
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4) 
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
```

* * *

1. Load data
essay: df of the raw data
grades: grade evaluated by different raters
char: characteristics of each work

```{r}
essay=read.csv("../DeYoreoKottas_supp/essay.csv",header=FALSE)
grades=essay[,1:5]
char=essay[,7:12]
names(char)=c("WL","SqW","PC","PS","PP","SL")
```

2. Extract features
p: Number of X (Covariates)
R: Number of Y (ordinal variables)
n: number of samples
x: df of covariate X
y: df of ordinal var Y
```{r}
p=0
R=3
d=p+R

n=nrow(essay)

y=matrix(0,n,R)

y[,1]=grades[,2]
y[,2]=grades[,3]
y[,3]=grades[,4]
```

3. Specify prior specification
N: Number of cluster in primary stick breaking process (DP)
R.x: Range of each covariates
mid.x: mid point of each covariates (max+min)/2
gamma1: cut off points of Y1
gamma2: cut off points of Y2
gamma3: cut off points of Y3
K: number of groups in ordinal data (3 levels: low med high)
R.z: range of z
```{r}
####prior specification####
N=40

gamma1=c(-10000,-4:4,10000)
gamma2=c(-10000,-4:4,10000)
gamma3=c(-10000,-4:4,10000)

K=length(gamma1)-1
R.z=c(gamma1[K]-gamma1[2],gamma2[K]-gamma2[2],gamma3[K]-gamma3[2])
```

4. Specify prior hyperparameters
```{r}
#Hyperparameters

# Prior for alpha = gamma(a.alpha, b.alpha)
a.alpha=.5
b.alpha=.5

# Prior for NIW distribution on DP
# N(mu| m, V)IW(sigma|v, S)

# m - N(a.m, B.m)
a.m=c(rep(0,3))
B.m=diag((R.z/4)^2,d)/2

# V - IW(a.V, B.V)
a.V=d+2
B.V=(a.V-d-1)*diag((R.z/4)^2,d)/2

# S - W(a.S, B.S), fix nu = d+2
a.S=d+2
nu=d+2
B.S=(nu-d-1)*(diag((R.z/4)^2,d))/(2*a.S)
```

5. Matrix for storing MCMC samples
Burn-in: 1000
Mon: sample = 5000 after thining
```{r}
#####store parameters####
Mon=500 # Final samples
B=50 # Burn-in
thin.int=10 # Thininig
Mon.true=(Mon)*thin.int # True number of samples after thining

k.mu=matrix(0,(Mon)*(d),N)
k.sigma=matrix(0,(Mon)*(d),N*(d))
L=matrix(0,Mon,n) # Cluster assignment
p.probs=matrix(0,Mon,N) # marginal prob of each cluster
alpha=rep(0,Mon) # Update alpha
# for NIW parameter update
M=matrix(0,Mon,d) # m
V=matrix(0,(Mon)*(d),d) # V
S=matrix(0,(Mon)*(d),d) # S

components=rep(0,Mon)
z1=matrix(0,Mon,n)
z2=matrix(0,Mon,n)
z3=matrix(0,Mon,n)
```

6. Specify initial values to start the chain

```{r}
####Initial Values####

# initialize mean for each cluster 0 for muY, mid of X for muX
k.mu.init=matrix(0,nrow=d,ncol=N)

# initialize covariance matrix for each cluster
k.sigma.init=matrix(0,nrow=d,ncol=N*(d))
num=seq(from=1,to=ncol(k.sigma.init)-(d-1),by=(d))
for(i in 1:(d))
k.sigma.init[i,num+(i-1)]=1

# sd = range/6
k.sigma.init[1,]=(R.z[1]/6)^2*k.sigma.init[1,]/2
k.sigma.init[2,]=(R.z[2]/6)^2*k.sigma.init[2,]/2
k.sigma.init[3,]=(R.z[2]/6)^2*k.sigma.init[3,]/2

# everyone is in the same cluster
L.init=rep(1,n) 

# equal probability for each cluster
p.probs.init=rep(1/N,N)

# alpha in DP is 1
alpha.init=1

# mean of mean in the NIW
M.init=c(0,0,0)
# covariance for mean in the NIW
V.init=diag(rep(1,d))
diag(V.init)=c((R.z/6)^2/5)

# initial guess for covariance matrix in NIW
S.init=k.sigma.init[1:d,1:d]*(nu-1)

components.init=1

# zi = (gamma[i] + gamma[i+1])/2
z.init=matrix(0,n,R)
z.init[which(y==1)]=gamma1[2]-1
z.init[which(y==K)]=gamma1[K]+1
for(i in 2:(K-1)){
z.init[which(y==i)]=(gamma1[i]+gamma1[i+1])/2
}

# save current value
k.mu.cur=k.mu.init
k.sigma.cur=k.sigma.init
L.cur=L.init
p.probs.cur=p.probs.init
alpha.cur=alpha.init
M.cur=M.init
V.cur=V.init
S.cur=S.init
components.cur=components.init
z.cur=z.init

row.cur=0
```

7. Perform MCMC (Gibbs sampling)

```{r}
####MCMC####
set.seed(0)
for(m in 2:(Mon.true+B+1)){
  
  # If pass burn-in and not thinned -> save
  if(m>(B+1)&&(m-B-1)%%thin.int==0) {row.cur=row.cur+1; save=1} else {save=0}
  
  # N.j: how many members are in each cluster
  N.j=numeric(N)
  for(i in 1:N)
  N.j[i]=sum(L.cur==i)
  
  # Calculate number of occupied clusters
  nstar=sum(N.j!=0)
  
  # occupied cluster and number of members in those occupied
  L.j=sort(unique(L.cur))
  N.j.star=N.j[which(N.j!=0)]
  
  # indices is a matrix that store the index of samples that fall into each clusters
  # the size of indices will be (No. used clusters x max number of members)
  Indices=matrix(0,nstar,max(N.j))
  for(i in 1:nstar)
  Indices[i,1:N.j.star[i]]=which(L.cur==L.j[i])
  
  ## step1: sample mu and sigma for each cluster
  # r is the index for used cluster
  # i is the index for all cluster
  # j is the index for members in that cluster
  r=1
  for(i in 1:N){
    if(N.j[i]==0){
      
      # No member in that cluster
      # draw mu, sigma from the base distribution
      k.mu.cur[,i]=rmvnorm(1, M.cur, V.cur)
      k.sigma.cur[,((i-1)*(d)+1):(i*(d))]=riwish(nu, S.cur)
      
      if(save==1){
        # save mu and sigma for empty clusters
        k.mu[((row.cur-1)*(d)+1):(row.cur*(d)),i]=k.mu.cur[,i]
        k.sigma[((row.cur-1)*(d)+1):(row.cur*(d)),((i-1)*(d)+1):(i*(d))]=k.sigma.cur[,((i-1)*(d)+1):(i*(d))] 
      }
      
    }else{
      # that cluster is occupied
      sum.zx=rep(0,d)
      sum.sigma.zx=matrix(0,d,d)
      
      for(j in 1:N.j[i]){
        # sum up the response vector of all members in that cluster
        sum.zx=sum.zx+c(z.cur[Indices[r,j],])
      }
      
      # update prior parameter for mu - N(m, V)
      # updated covariance matrix = prior precision + sample precision
      b.star.mu=solve(solve(V.cur)+N.j[i]*solve(k.sigma.cur[,((i-1)*(d)+1):(i*(d))]))
      # updated mean vector
      a.star.mu=solve(V.cur)%*%matrix(data=M.cur,nrow=d,ncol=1)+solve(k.sigma.cur[,((i-1)*(d)+1):(i*(d))])%*%sum.zx
      # draw mu from updated distribution
      k.mu.cur[,i]=rmvnorm(1,b.star.mu%*%a.star.mu,b.star.mu)
      
      # calculate Stheta = sum[(zj - mu)(zj-mu)^T]
      for(j in 1:N.j[i]){
        diff=matrix(data=c(z.cur[Indices[r,j],])-k.mu.cur[,i],nrow=d,ncol=1) # dx1 vector
        sum.sigma.zx=sum.sigma.zx+diff%*%t(diff) 
      }
      # draw sigma for that cluster from IW distribution
      k.sigma.cur[,((i-1)*(d)+1):(i*(d))]=riwish(nu+N.j[i],S.cur+sum.sigma.zx)
      
      r=r+1 # complete this used cluster -> move to the next
      
      if(save==1){
        # save sampled mu and sigma for that cluster
        k.mu[((row.cur-1)*(d)+1):(row.cur*(d)),i]=k.mu.cur[,i]
        k.sigma[((row.cur-1)*(d)+1):(row.cur*(d)),((i-1)*(d)+1):(i*(d))]=k.sigma.cur[,((i-1)*(d)+1):(i*(d))]
      }
    }
  }
  
  #########################################################################
  ## step2: sample L
  
  L.cur=numeric(n)
  
  # Collect cluster mean and sigma sampled from step1 into meancov.list
  meancov.list=vector("list",N)
  for(j in 1:N){
    meancov.list[[j]]=list(k.mu.cur[,j],k.sigma.cur[,((j-1)*(d)+1):(j*(d))])
  }
  
  # Calculate p(Z|mu_l, Sigma_l) for each sample for each cluster = p(Z|L)
  norm.dens.list=lapply(meancov.list, function( item ) dmvnorm (z.cur, item[[1]], item[[2]]))
  # Turn that into a matrix of size 198x40: ij entry is p(Zi|L = j)
  norm.dens.mtrx=NULL
  for(j in 1:N){
    norm.dens.mtrx=cbind(norm.dens.mtrx,as.numeric(norm.dens.list[[j]]))
  }
  
  # Sample new cluster assignment from P(L|z) = P(z|L)P(L)
  for(i in 1:n){
    L.cur[i]=sample.int(N,size=1,prob=norm.dens.mtrx[i,]*p.probs.cur)
  }
  if(save==1){
    L[row.cur,]=L.cur
  }
  
  #update components: Number of occupied clusters
  components.cur=length(unique(L.cur))
  if(save==1)
  components[row.cur]=components.cur
  
  #########################################################################
  ## step3: Sample p.probs
  
  # Calculate Nj (Don't know why they don't use Nj)
  M.config=numeric(N)
  for(i in 1:N)
  M.config[i]=sum(L.cur==i)
  
  # Update stick breaking process: Vi
  vstar=numeric(N-1)
  for(i in 1:(N-1)){
    vstar[i]=rbeta(1,1+M.config[i],alpha.cur+sum(M.config[(i+1):N]))
    if(vstar[i]==0) vstar[i]=.00001
    if(vstar[i]==1) vstar[i]=.9999
  }
    
  # Update pi for each cluster (mixing proportion)
  # Using updated Vi and stick breaking process
  p.probs.cur[1]=vstar[1]
  prod=1
  for(i in 2:(N-1)){
    prod=prod*(1-vstar[i-1])
    p.probs.cur[i]=vstar[i]*prod
  }
  
  if((1-sum(p.probs.cur[1:(N-1)]))>=0)
  p.probs.cur[N]=1-sum(p.probs.cur[1:(N-1)]) else p.probs.cur[N]=0
  if(save==1)
  p.probs[row.cur,]=p.probs.cur
  
  #########################################################################
  # step4: sample alpha
  
  # sample new alpha (for stick breaking) from the posterior gamma dist
  alpha.cur=rgamma(1,a.alpha+N-1,rate= -sum(log(1-vstar))+b.alpha)
  if(save==1)
  alpha[row.cur]=alpha.cur
  
  #########################################################################
  # step5: sample M and V (parameter for DP NIW)
  
  # sample M from normal dist
  # Update covariance matrix
  B.m.star=solve(solve(B.m)+N*solve(V.cur))
  # Update mean vector
  sum=0
  for(i in 1:N)
  sum=sum+solve(V.cur)%*%k.mu.cur[,i]
  a.m.star=B.m.star%*%(solve(B.m)%*%a.m+sum)
  # Sample M from full conditionally
  M.cur=rmvnorm(1,a.m.star,B.m.star)
  if(save==1)
  M[row.cur,]=M.cur
  
  
  # sample V from IW
  sum=matrix(0,nrow=d,ncol=d)
  for(i in 1:N){
    # diff = mu-M
    mu.diff=matrix(data=k.mu.cur[,i]-M.cur,nrow=d,ncol=1)
    # calculate sum(mu-M)(mu-M)^T
    sum=sum+mu.diff%*%t(mu.diff)
  }
  V.cur=riwish(a.V+N,B.V+sum)
  
  if(save==1)
  V[((row.cur-1)*(d)+1):(row.cur*(d)),]=V.cur
  
  #########################################################################
  # step6: sample S (parameter for DP NIW)
  # The posterior of S will be Wishart
  sum.S=matrix(0,d,d)
  
  for(i in 1:N)
  sum.S=sum.S+solve(k.sigma.cur[,((i-1)*(d)+1):(i*(d))])
  
  S.cur=rwish(a.S+nu*N,solve(solve(B.S)+sum.S))
  if(save==1) S[((row.cur-1)*(d)+1):(row.cur*(d)),]=S.cur
  
  
  #########################################################################
  # step7: sample z
  for(i in 1:n){
    # impute z1 given z2, z3 and X
    mu.z1=k.mu.cur[1,L.cur[i]]
    sig=k.sigma.cur[,((L.cur[i]-1)*(d)+1):((L.cur[i])*(d))]
    
    sig.zx=matrix(data=sig[1,-1],nrow=1,ncol=d-1)
    sig.xz=t(sig.zx)
    sig.xx=matrix(data=sig[-1,-1],nrow=d-1,ncol=d-1)
    sig.zz=sig[1,1]
    diff=matrix(data=c(z.cur[i,-1])-k.mu.cur[-1,L.cur[i]],nrow=d-1,ncol=1)
    
    # calculate conditional mean and variance for z1
    m.z=mu.z1+sig.zx%*%solve(sig.xx)%*%diff
    s.z=sig.zz-sig.zx%*%solve(sig.xx)%*%sig.xz
    # Sample z1 from truncated normal considering yi
    z.cur[i,1]=rtruncnorm(1,a=gamma1[y[i,1]],b=gamma1[y[i,1]+1],mean=m.z,sd=sqrt(s.z))
    
    # impute z2 given z1, z3 and X
    mu.z2=k.mu.cur[2,L.cur[i]]
    
    sig.zx2=matrix(data=sig[2,-2],nrow=1,ncol=d-1)
    sig.xz2=t(sig.zx2)
    sig.xx2=matrix(data=sig[-2,-2],nrow=d-1,ncol=d-1)
    sig.zz2=sig[2,2]
    diff2=matrix(data=c(z.cur[i,-2])-k.mu.cur[-2,L.cur[i]],nrow=d-1,ncol=1)
    
    # calculate conditional mean and variance for z2
    m.z2=mu.z2+sig.zx2%*%solve(sig.xx2)%*%diff2
    s.z2=sig.zz2-sig.zx2%*%solve(sig.xx2)%*%sig.xz2
    # Sample z2 from truncated normal considering yi
    z.cur[i,2]=rtruncnorm(1,a=gamma2[y[i,2]],b=gamma2[y[i,2]+1],mean=m.z2,sd=sqrt(s.z2))
    
    # Do the same for z3
    mu.z3=k.mu.cur[3,L.cur[i]]
    
    sig.zx3=matrix(data=sig[3,-3],nrow=1,ncol=d-1)
    sig.xz3=t(sig.zx3)
    sig.xx3=matrix(data=sig[-3,-3],nrow=d-1,ncol=d-1)
    sig.zz3=sig[3,3]
    diff3=matrix(data=c(z.cur[i,-3])-k.mu.cur[-3,L.cur[i]],nrow=d-1,ncol=1)
    
    m.z3=mu.z3+sig.zx3%*%solve(sig.xx3)%*%diff3
    s.z3=sig.zz3-sig.zx3%*%solve(sig.xx3)%*%sig.xz3
    
    z.cur[i,3]=rtruncnorm(1,a=gamma3[y[i,3]],b=gamma3[y[i,3]+1],mean=m.z3,sd=sqrt(s.z3))
  
  }
  if(save==1){
  z1[row.cur,]=z.cur[,1]
  z2[row.cur,]=z.cur[,2]
  z3[row.cur,]=z.cur[,3]
  }
  if(save==1) print(c(row.cur,m,components.cur))

}

```

```{r}
head(z1)
```

```{r}
head(z2)
```

```{r}
head(z3)
```

```{r}
sum(z1)
sum(z2)
sum(z3)
```

* * *



