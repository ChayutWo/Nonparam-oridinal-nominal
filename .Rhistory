qt(0.975, df = 42)
qt(0.975, df = 43)
1.6825/1000
fisher.test(c(0.85, 0.99, 1, 0.76, 0.26, 0.45, 0.97, 0.72),y = c(1,1,1,1,0,0,0,0))
install.packages("combinat")
library(combinat)
data <- c(85,99,100,76,26,45,97,72)
base <- average(data[1:4] - data[5:8])
base <- mean(data[1:4] - data[5:8])
base
permn(data)
?permn
permn(c(1,2,3))
permn(c(84, 85))
permn(data)
8*7*6*5*4*3*2
base
count = 0
ok = 0
for(dat in permn(data)){
if(mean(dat[1,4]-dat[5-8]) >=30 or mean(dat[1,4]-dat[5-8]) <= -30){
ok = ok+1
}
count = count+1
}
print(ok/count)
count = 0
ok = 0
for(dat in permn(data)){
check = mean(dat[1,4]-dat[5-8])
if(check >=30 or check <= -30){
ok = ok+1
}
count = count+1
}
print(ok/count)
count = 0
ok = 0
for (dat in permn(data)) {
check = mean(dat[1,4]-dat[5-8])
if (check >=30 or check <= -30) {
ok = ok+1
}
count = count+1
}
print(ok/count)
for (dat in permn(data)) {
check = mean(dat[1,4]-dat[5-8])
if (check >=30 or check <= -30) {
ok = ok+1
}
count = count+1
}
print(ok/count)
for (dat in permn(data)) {
check = mean(dat[1,4]-dat[5-8])
if (check >=30 or check <= -30) {ok = ok+1}
count = count+1
}
print(ok/count)
FALSE|TRUE
count = 0
ok = 0
for (dat in permn(data)) {
check = mean(dat[1,4]-dat[5-8])
if (check >=30 | check <= -30) {
ok = ok+1
}
count = count+1
}
print(ok/count)
count = 0
ok = 0
for (dat in permn(data)) {
check = mean(dat[1:4]-dat[5:8])
if (check >=30 | check <= -30) {
ok = ok+1
}
count = count+1
}
print(ok/count)
qnorm(0.95)
-0.0081 + qnorm(0.95)*0.0024
-0.0081 - qnorm(0.95)*0.0024
qnorm(0.95)
df<- read.csv("C:\\Users\\ChayutWo\\Downloads\\qian.csv")
df<- read.csv("C:\\Users\\ChayutWo\\Downloads\\qian.csv")
view(df)
View(df)
mean(df$biryr)
summary(df)
df$sex <- as.factor(df$sex)
df$sex <- as.factor(df$sex)
summary(df)
df<- read.csv("C:\\Users\\ChayutWo\\Downloads\\qian.csv")
summary(df)
df$post <- as.factor(df$biryr> 1979)
df$post <- as.factor(df$biryr>= 1979)
sum(df$post)
sum(df$biryr>= 1979)
df<- read.csv("C:\\Users\\ChayutWo\\Downloads\\qian.csv")
df$post <- as.numeric(df$biryr>= 1979)
df$postTea <- df$post*df$teasown
df$post <- as.factor(df$post)
mean(df$postTea)
colnames(df)
lm(sex~teasown+post+postTea, data = df)
mod1 <-lm(sex~teasown+post+postTea, data = df)
summary(mod1)
mod2 <- lm(sex~post+postTea, data = df)
summary(mod2)
mod1 <-lm(sex~teasown+post+post*teasown, data = df)
summary(mod1)
library(ISLR)
fix(Hitters)
names(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
library(leaps)
regfit.full <- regsubsets(Salary~., data =Hitters, nvmax = 19)
summary(regfit.full)
reg.summary <- summary(regfit.full)
par(mfrow=c(2,2)
par(mfrow=c(2,2))
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l")
points (11,reg. summary$adjr2[11], col="red",cex=2,pch =20)
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l")
points (11,reg. summary$adjr2[11], col="red",cex=2,pch =20)
points (11,reg.summary$adjr2[11], col="red",cex=2,pch =20)
plot(regfit.full ,scale="adjr2")
plot(regfit.full ,scale="adjr2")
plot(regfit.full ,scale="adjr2")
par(mfrow=c(1,1))
plot(regfit.full ,scale="adjr2")
plot(regfit.full ,scale="bic")
coef(regfit.full, 6)
train = sample(c(TRUE, FALSE), nrow(Hitters), rep= TRUE)
test = !train
regfit.best=regsubsets (Salary∼.,data=Hitters[train ,],
nvmax=19)
test.mat = model.matrix(Salary~., data = Hitters[test,])
val.errors = rep(NA, 19)
regfit.best=regsubsets (Salary∼.,data=Hitters[train ,],
nvmax=19)
test.mat = model.matrix(Salary~., data = Hitters[test,])
val.errors = rep(NA, 19)
for(i in 1:19){
coefi = coef(regfit.best, i)
pred = test.mat[, names(coefi)] %*% coefi
val.erros[i] = mean((Hitters$Salary[test]-pred)^2)
}
test.mat = model.matrix(Salary~., data = Hitters[test,])
val.errors = rep(NA, 19)
for(i in 1:19){
coefi = coef(regfit.best, i)
pred = test.mat[, names(coefi)] %*% coefi
val.errors[i] = mean((Hitters$Salary[test]-pred)^2)
}
install.packages("glmnet")
library(glmnet)
x = model.matrix(Salary~., Hitters)
x
x = model.matrix(Salary~., Hitters)[, -1]
x
x = model.matrix(Salary~., Hitters)
names(x)
str(x)
x = model.matrix(Salary~., Hitters)[, -1]
y = Hitters$Salary
grid = 10^seq(10, -2, length=  100)
ridge.mod <- glmnet(x,y,alpha = 0, lambda = grid)
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = -train
y.test = y[test]
ridge.mod <- glmnet(x[train,], y[train]), alpha = 0,lambda =grid, thresh = 1e-12)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0,lambda =grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
ridge.pred <- predict(ridge.mod, s = cv.out$lambda.min, newx = x[test,])
mean((ridge.pred - y.test)^2)
install.packages('pls')
library(ISLR)
fix(Hitters)
names(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
library(leaps)
regfit.full <- regsubsets(Salary~., data =Hitters, nvmax = 19)
reg.summary <- summary(regfit.full)
par(mfrow=c(1,1))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l")
points (11,reg.summary$adjr2[11], col="red",cex=2,pch =20)
plot(regfit.full ,scale="bic")
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(Hitters), rep= TRUE)
test = !train
regfit.best=regsubsets (Salary∼.,data=Hitters[train ,],
nvmax=19)
test.mat = model.matrix(Salary~., data = Hitters[test,])
val.errors = rep(NA, 19)
for(i in 1:19){
coefi = coef(regfit.best, i)
pred = test.mat[, names(coefi)] %*% coefi
val.errors[i] = mean((Hitters$Salary[test]-pred)^2)
}
install.packages("glmnet")
library(glmnet)
x = model.matrix(Salary~., Hitters)[, -1]
y = Hitters$Salary
grid = 10^seq(10, -2, length=  100)
ridge.mod <- glmnet(x,y,alpha = 0, lambda = grid)
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = -train
y.test = y[test]
ridge.mod <- glmnet(x[train,], y[train], alpha = 0,lambda =grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
ridge.pred <- predict(ridge.mod, s = cv.out$lambda.min, newx = x[test,])
mean((ridge.pred - y.test)^2)
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
set.seed(0)
cv.out<- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
lasso.pred <- predict(lasso.mod, s = cv.out$lambda.min, newx= x[test,])
mean((lasso.pred - y.test)^2)
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = cv.out$lambda.min)[1:20,]
lasso.coef
library(pls)
pcr.fit <- pcr(Salary~., data = Hitters, scale = TRUE, validation = 'CV')
summary(pcr.fit)
validationplot(pcr.fit, val.type = 'MSEP')
set.seed(1)
pcr.fit<-pcr(Salary~., data = Hitters, subset = train, sclae = TRUE, validation = 'CV')
validationplot(pcr.fit, val.type = 'MSEP')
pcr.pred = predict(pcr.fit, x[test,], ncomp = 7)
mean((pcr.pred - y[test])^2)
mean((pcr.pred - y.test)^2)
pcr.fit<-pcr(Salary~., data = Hitters, scale = TRUE, ncomp = 7)
pcr.fit<-pcr(Salary~., data = Hitters, subset = train, scale = TRUE, validation = 'CV')
validationplot(pcr.fit, val.type = 'MSEP')
pcr.pred = predict(pcr.fit, x[test,], ncomp = 7)
mean((pcr.pred - y.test)^2)
pcr.fit<-pcr(Salary~., data = Hitters, scale = TRUE, ncomp = 7)
summary(pcr.fit)
set.seed(1)
pls.fit <- plsr(Salary~., data = Hitters, subset = train, scale = TRUE, validation = 'CV')
summary(pls.fit)
pls.pred <-predict(pls.fit, x[test,], ncomp = 2)
mean((pls.pred - y.test)^2)
install.packages('tree')
library(tree)
library(ISLR)
attach(Carseats)
High = ifelse(Sales<=8, 'No', 'Yes')
Carseats = data.frame(Carseats, High)
tree <- tree(High~.-Sales, data = Carseats)
summary(tree)
plot(tree)
text(tree, pretty = 0)
tree
set.seed(2)
train = sample(1:nrow(Carseats), 200)
Carseats.test = Carseats[-train,]
High.test = High[-train]
tree <- tree(High~.-Sales, data = Carseats, subset = train)
tree.pred = predict(tree, Carseats.test, type = 'Class')
tree.pred = predict(tree, Carseats.test, type = 'class')
table(tree.pred, High.test)
set.seed(3)
cv.car = cv.tree(tree, FUN = prune.misclass())
cv.car = cv.tree(tree, FUN = prune.misclass
cv.car = cv.tree(tree, FUN = prune.misclass)
cv.car = cv.tree(tree, FUN = prune.misclass)
names(cv.car)
cv.car
par(mfrow = c(1,2))
plot(cv.car$sizw, cv.car$dev, type = 'b')
plot(cv.car$size, cv.car$dev, type = 'b')
prune.car = prune.misclass(tree, best= 9)
plot(prune.car)
text(prune.car, pretty = 0)
tree.pred = predict(prune.car, Carseats.test, type = 'class')
table(tree.pred, High.test)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston = tree(medv~., Boston, subset = train)
summary(tree.boston)
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev)
library(randomForest)
set.seed(1)
bag.boston = randomForest(medv~., data = Boston, subset = train, mtry = 13, importance = TRUE)
bag.boston
yhat.bag = predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, Boston[-train, 'medv'])
importance(bag.boston)
varImpPlot(bag.boston)
install.packages(gbm)
install.packages('gbm')
library(gbm)
set.seed(1)
boost.boston = gbm(medv~., data = Boston[train,], distribution = 'gaussian', n.trees = 5000, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston, i= 'rm')
library(ISLR)
nci.labs = NCI60$labs
nci.data = NCI60$data
nci.labs[1:4]
pr.out = prcomp(nci.data, scale = TRUE)
pr.out = prcomp(nci.data, scale = TRUE)
Cols=function (vec){
cols=rainbow (length(unique(vec)))
return(cols[as.numeric (as.factor(vec))])
}
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z2")
plot(pr.out)
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z3")
sd.data = scale(nci.data)
data.dist = dist(sd.data)
plot(hclust(data.dist, method = 'average'), lavels = nci.labs)
nci.labs
nci.labs
par(mfrow=c(1,1))
plot(hclust(data.dist, method = 'average'), lavels = nci.labs)
plot(hclust(data.dist, method = 'average'), labels = nci.labs)
hc.out = hclust(data.dist)
hc.cluster = cutree(hc.out, 4)
table(hc.cluster, nci.labs)
summary(hc.out)
set.seed(2)
km.out = kmeans(sd.data, 4, nstart = 20)
km.cluster = km.out$cluster
table(km.cluster, hc.cluster)
hc.out = hclust(dist(pr.out$x[,1:5]))
plot(hc.out, labels = nci.labs)
O <- 23.74/33.84
nonO <- (100-23.74)/(100-33.84)
O/nonO
nonO <- (100-25.74)/(100-33.84)
O <- 25.74/33.84
O/nonO
prob =c(0.3384, 1-0.3384)
chisq.test(x = c(486, 1888-486), p = prob)
chisq.test(x = c(486, 1888-486), p = prob)
prob =c(0.0910, 1-0.0910)
chisq.test(x = c(193, 1888-193), p = prob)
chisq.test(x = c(25.74, 33.84), p = prob)
prob =c(0.3216, 0.2490, 0.0910, 0.3384)
chisq.test(x = c(715, 494, 193, 486), p = prob)
17.88+0.983+1.72+37.852
chisq.test(x = c(85, 50, 19, 52), p = prob)
prob
prob2 = c(0.3775, 0.2642, 0.1003, 0.2580)
chisq.test(x = c(85, 50, 19, 52), p = prob2)
sigma<-1/(2*pi)
sigma
sigma<-sqrt(1/(2*pi))
load('../Datasets/ordinalPUMS.Rdata')
load('../ordinalPUMS.Rdata')
setwd("D:/DS_Project/Nonparam-oridinal-nominal")
load('../ordinalPUMS.Rdata')
load('..//ordinalPUMS.Rdata')
load('..\\ordinalPUMS.Rdata')
load('..\ordinalPUMS.Rdata')
load('../ordinalPUMS.Rdata')
load('ordinalPUMS.Rdata')
source("probitBayes.R")
install.packages('knitr')
install.packages('dplyr')
install.packages('arm')
install.packages('pROC')
install.packages('tidyverse')
install.packages('lme4')
install.packages('lattice')
install.[a]
install.packages('broom')
install.packages('boot')
install.packages('rstanarm')
install.packages('magrittr')
install.packages('rstan')
install.packages('MCMCpack')
install.packages('abind')
install.packages('matrixStats')
install.packages('truncnorm')
install.packages('mvtnorm')
install.packages('mnormt')
install.packages('coda')
# DO NOT ALTER CODE IN THIS CHUNK
# The very first time you use this R markdown file, you should install each of the packages below.
# The same goes for other packages you might decide to use
# Remember that you only need to install each R package once in R (ever -- unless you change computers).
# All you need to do whenever you need to use the package again (after restarting the R session),
# is to use the library function to call the package.
# For example, type install.packages("knitr") in the console to install the knitr package.
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4)
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(mnormt)
library(coda)
# load dataset: df
load('ordinalPUMS.Rdata')
setwd("D:/DS_Project/Nonparam-oridinal-nominal")
# load dataset: df
load('ordinalPUMS.Rdata')
load('ordinalPUMS.Rdata')
# load dataset: df
load('.//ordinalPUMS.Rdata')
# load dataset: df
load('./ordinalPUMS.Rdata')
# load dataset: df
load('../ordinalPUMS.Rdata')
# take 10,000 samples: df
n = 10000
sample <- sample(nrow(df), size = 10000)
df <- df[sample,]
# create MCAR scneario with 30% chance of missing: df_observed
missing_prob = 0.3
df_observed <- df
missing_col = colnames(df)[c(1,3,5,7,9,11)]
for (col in missing_col) {
missing_ind <- rbernoulli(n,p = missing_prob)
df_observed[missing_ind, col] <- NA
}
# load dataset: df
load('../ordinalPUMS.Rdata')
# take 10,000 samples: df
n = 10000
sample <- sample(nrow(df), size = 10000)
df <- df[sample,]
# create MCAR scneario with 30% chance of missing: df_observed
missing_prob = 0.3
df_observed <- df
missing_col = colnames(df)[c(1,3,5,7,9,11)]
for (col in missing_col) {
missing_ind <- rbernoulli(n,p = missing_prob)
df_observed[missing_ind, col] <- NA
}
# load dataset: df
load('../ordinalPUMS.Rdata')
# take 10,000 samples: df
n = 10000
sample <- sample(nrow(df), size = 10000)
df <- df[sample,]
# create MCAR scneario with 30% chance of missing: df_observed
set.seed(0)
missing_prob = 0.3
df_observed <- df
missing_col = colnames(df)[c(1,3,5,7,9,11)]
for (col in missing_col) {
missing_ind <- rbernoulli(n,p = missing_prob)
df_observed[missing_ind, col] <- NA
}
source("probitBayes.R")
source("../probitBayes.R")
N = 40
Mon = 100
B = 5
thin.int = 2
# function(y, N = 40, Mon = 2000, B = 300, thin.int = 5, seed = 0)
sampled_y <- probitBayesImputation(df_observed, N, Mon, B, thin.int)
View(df)
