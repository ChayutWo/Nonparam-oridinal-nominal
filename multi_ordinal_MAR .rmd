
---
title: "Make the code suitable for any number of ordinal variables"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: spacelab
---

```{r setup, echo =FALSE, include=FALSE}
# DO NOT ALTER CODE IN THIS CHUNK
# The very first time you use this R markdown file, you should install each of the packages below.
# The same goes for other packages you might decide to use
# Remember that you only need to install each R package once in R (ever -- unless you change computers).
# All you need to do whenever you need to use the package again (after restarting the R session),
# is to use the library function to call the package.
# For example, type install.packages("knitr") in the console to install the knitr package. 
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4) 
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
```

* * *

1. Simulate data and set cutoffs

Setting:

Generate $Z \sim Normal(\mu, \Sigma)$ where:

- $\mu$ = $[0, 0, 0, 0]$

- $$\Sigma = (\begin{array}{cc} 
4 & 0.4 & -2.4 & 1.6\\
0.4 & 4 & 1.2 & -2.4\\
-2.4 & 1.2 & 9 & 1.2\\
1.6 & -2.4 & 1.2 & 4
\end{array})$$

- From $z_1$ to $z_4$ apply thresholds to get $y_1$ to $y_4$

- $y_1$ and $y_3$ will have 5 levels while the others have 4

```{r}
# Set mean and covariance matrix for z
set.seed(0)
n <- 500
R <- 4
d <- 4

true_mu <- c(0, 0, 0, 0)
true_sigma <- matrix(c(4, 0.4, -2.4, 1.6, 0.4, 4,1.2,
                      -2.4,-2.4,1.2,9,1.2,1.6,-2.4,1.2,4), nrow = 4, ncol = 4)

# sample Z from multivariate normal
Z <- rmvnorm(n, mean = true_mu, sigma = true_sigma)

# set cutoffs
levels <- c(5,4,5,4)
# z1 will have 5 levels
gamma1_true <- quantile(Z[,1], probs = c(0.15, 0.3, 0.7, 0.85))
# z2 will have 4 levels
gamma2_true <- quantile(Z[,2], probs = c(0.25, 0.5, 0.75))
# z3 will have 5 levels
gamma3_true <- quantile(Z[,1], probs = c(0.15, 0.35, 0.7, 0.85))
# z4 will have 4 levels
gamma4_true <- quantile(Z[,2], probs = c(0.2, 0.55, 0.8))

gamma_true <- list(gamma1_true, gamma2_true, gamma3_true, gamma4_true)

# Set y according to the cutoff
y_original <- matrix(0, nrow = n, ncol = R)
for (z_index in 1:R) {
  yval <- 2
  for (level in 1:(levels[z_index]-1)) {
    ind <- (Z[,z_index] <= gamma_true[[z_index]][level+1]) & 
      (Z[,z_index] > gamma_true[[z_index]][level])
    
    y_original[ind,z_index] <- yval
    yval <- yval + 1
  }
  # Last level
  ind <- Z[,z_index] > gamma_true[[z_index]][levels[z_index]-1]
  y_original[ind,z_index] <- levels[z_index]
  # First level
  ind <- Z[,z_index] <= gamma_true[[z_index]][1]
  y_original[ind,z_index] <- 1
}
```

2. Making missing data and setting indicator matrix

About 33% of $y_3$ and $y_4$ will be missing according to MAR.

```{r}
set.seed(0)
# Make Z3 and Z4 MAR
funcZ3 <- exp(Z[, 1] - 0.5*Z[, 2]-1.25)
funcZ4 <- exp(-Z[, 1] + 0.5*Z[, 2]-1.25)
missing_probZ3 <- funcZ3/(funcZ3 + 1)
missing_probZ4 <- funcZ4/(funcZ4 + 1)

# Create the missing indicator vector
indicatorZ3 <- rbernoulli(n = n, p = missing_probZ3)
indicatorZ4 <- rbernoulli(n = n, p = missing_probZ4)
IndicatorMat <- cbind(rep(0,n), rep(0,n), indicatorZ3, indicatorZ4)

# Create y matrix to be used in MCMC, impute missing value by 2
y <- y_original
y[indicatorZ3,3] <- 2
y[indicatorZ4,4] <- 2
mean(indicatorZ3)
mean(indicatorZ4)
barplot(table(y_original[indicatorZ3,3])/length(y_original[indicatorZ3,3]), 
        main = 'Missing data distribution in y3')
barplot(table(y_original[indicatorZ4,4])/length(y_original[indicatorZ4,4]), 
        main = 'Missing data distribution in y4')
```
```{r}
# Distribution of observed data
barplot(table(y_original[!indicatorZ3,3])/length(y_original[!indicatorZ3,3]), 
        main = 'Missing data distribution in y3')
barplot(table(y_original[!indicatorZ4,4])/length(y_original[!indicatorZ4,4]), 
        main = 'Missing data distribution in y4')
```

3. Specify prior specification
N: Number of cluster in primary stick breaking process (DP)
gamma: cutoff points - can be arbitrary
K: number of groups in ordinal data (3 levels: low med high)
R.z: range of z

```{r}
####prior specification####
N=40 # number of initial clusters

# The cutoffs according to this paper can be arbitrarily set
gamma1=c(-10000,-1,0,1,2,10000)
gamma2=c(-10000,-1,0,1,10000)
gamma3=c(-10000,-1,0,1,2,10000)
gamma4=c(-10000,-1,0,1,10000)
gamma.list = list(gamma1, gamma2, gamma3, gamma4)
# >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<
R.z = rep(0, R)
for (z_index in 1:R) {
  Ki = length(gamma.list[[z_index]])-1
  R.z[z_index] = gamma.list[[z_index]][Ki] - gamma.list[[z_index]][2]
}
# >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<
```

4. Specify prior hyperparameters (default)

```{r}
#Hyperparameters

# Prior for alpha = gamma(a.alpha, b.alpha)
a.alpha=.5
b.alpha=.5

# Prior for NIW distribution on DP
# N(mu| m, V)IW(sigma|v, S)

# m - N(a.m, B.m)
a.m=rep(0,R)
B.m=diag(c((R.z/4)^2),d)/2

# V - IW(a.V, B.V)
a.V=d+2
B.V=(a.V-d-1)*diag(c((R.z/4)^2),d)/2

# S - W(a.S, B.S), fix nu = d+2
a.S=d+2
nu=d+2
B.S=(nu-d-1)*(diag(c((R.z/4)^2),d))/(2*a.S)
```

5. Matrix for storing MCMC samples
Burn-in: 1000
Mon: sample = 2000 after thining

```{r}
#####store parameters####
Mon=2000 # Final samples
B=300 # Burn-in
thin.int=5 # Thininig
Mon.true=(Mon)*thin.int # True number of samples after thining

k.mu=matrix(0,(Mon)*(d),N)
k.sigma=matrix(0,(Mon)*(d),N*(d))
L=matrix(0,Mon,n) # Cluster assignment
p.probs=matrix(0,Mon,N) # marginal prob of each cluster
alpha=rep(0,Mon) # Update alpha
# for NIW parameter update
M=matrix(0,Mon,d) # m
V=matrix(0,(Mon)*(d),d) # V
S=matrix(0,(Mon)*(d),d) # S

components=rep(0,Mon)

# >>>>>>>>>try to group z together<<<<<<<<<<<<
sampled_z = c()
sampled_y = c()
for (z_index in 1:R) {
  sampled_z = abind(sampled_z, matrix(0,Mon,n), along = 3)
  sampled_y = abind(sampled_y, matrix(0,Mon,n), along = 3)
}
# >>>>>>>>>try to group z together<<<<<<<<<<<<
```

6. Specify initial values to start the chain

```{r}
####Initial Values####

# initialize mean for each cluster 0 for muY
k.mu.init=matrix(0,nrow=d,ncol=N)

# initialize covariance matrix for each cluster
k.sigma.init=matrix(0,nrow=d,ncol=N*(d))
num=seq(from=1,to=ncol(k.sigma.init)-(d-1),by=(d))
for(i in 1:(d))
k.sigma.init[i,num+(i-1)]=1

# >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<
# sd = range/6
for (var_ind in 1:R) {
  k.sigma.init[var_ind,]=(R.z[var_ind]/6)^2*k.sigma.init[var_ind,]/2
}
# >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<

# everyone is in the same cluster
L.init=rep(1,n) 

# equal probability for each cluster
p.probs.init=rep(1/N,N)

# alpha in DP is 1
alpha.init=1

# >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<
# mean of mean in the NIW
M.init=rep(0, R)
# >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<

# covariance for mean in the NIW
V.init=diag(rep(1,d))
diag(V.init)=c((R.z/6)^2/5)

# initial guess for covariance matrix in NIW
S.init=k.sigma.init[1:d,1:d]*(nu-1)

components.init=1

# >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<
z.init=matrix(0,n,R)
for (z_index in 1:R) {
  Ki = length(gamma.list[[z_index]])-1
  # left end
  z.init[which(y[, z_index]==1), z_index]=gamma.list[[z_index]][2]-1
  # right end
  z.init[which(y[, z_index]==Ki), z_index]=gamma.list[[z_index]][Ki]+1
  # middle zi = (gamma[i] + gamma[i+1])/2
  for(i in 2:(Ki-1)){
    z.init[which(y[, z_index]==i),z_index]=(gamma.list[[z_index]][i]+
                                              gamma.list[[z_index]][i+1])/2
  }
}
# >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<

# save current value
k.mu.cur=k.mu.init
k.sigma.cur=k.sigma.init
L.cur=L.init
p.probs.cur=p.probs.init
alpha.cur=alpha.init
M.cur=M.init
V.cur=V.init
S.cur=S.init
components.cur=components.init
z.cur = z.init
y.cur = y
row.cur=0
```

7. Perform MCMC (Gibbs sampling)

```{r}
####MCMC####
set.seed(0)
for(m in 2:(Mon.true+B+1)){
  
  # If pass burn-in and not thinned -> save
  if(m>(B+1)&&(m-B-1)%%thin.int==0) {row.cur=row.cur+1; save=1} else {save=0}
  
  # N.j: how many members are in each cluster
  N.j=numeric(N)
  for(i in 1:N)
  N.j[i]=sum(L.cur==i)
  
  # Calculate number of occupied clusters
  nstar=sum(N.j!=0)
  
  # occupied cluster and number of members in those occupied
  L.j=sort(unique(L.cur))
  N.j.star=N.j[which(N.j!=0)]
  
  # indices is a matrix that store the index of samples that fall into each clusters
  # the size of indices will be (No. used clusters x max number of members)
  Indices=matrix(0,nstar,max(N.j))
  for(i in 1:nstar)
  Indices[i,1:N.j.star[i]]=which(L.cur==L.j[i])
  
  ## step1: sample mu and sigma for each cluster
  # r is the index for used cluster
  # i is the index for all cluster
  # j is the index for members in that cluster
  r=1
  for(i in 1:N){
    if(N.j[i]==0){
      
      # No member in that cluster
      # draw mu, sigma from the base distribution
      k.mu.cur[,i]=rmvnorm(1, M.cur, V.cur)
      k.sigma.cur[,((i-1)*(d)+1):(i*(d))]=riwish(nu, S.cur)
      
      if(save==1){
        # save mu and sigma for empty clusters
        k.mu[((row.cur-1)*(d)+1):(row.cur*(d)),i]=k.mu.cur[,i]
        k.sigma[((row.cur-1)*(d)+1):(row.cur*(d)),((i-1)*(d)+1):(i*(d))]=k.sigma.cur[,((i-1)*(d)+1):(i*(d))] 
      }
      
    }else{
      # that cluster is occupied
      sum.zx=rep(0,d)
      sum.sigma.zx=matrix(0,d,d)
      
      for(j in 1:N.j[i]){
        # sum up the response vector of all members in that cluster
        sum.zx=sum.zx+z.cur[Indices[r,j],]
      }
      
      # update prior parameter for mu - N(m, V)
      # updated covariance matrix = prior precision + sample precision
      b.star.mu=solve(solve(V.cur)+N.j[i]*solve(k.sigma.cur[,((i-1)*(d)+1):(i*(d))]))
      # updated mean vector
      a.star.mu=solve(V.cur)%*%matrix(data=M.cur,nrow=d,ncol=1)+solve(k.sigma.cur[,((i-1)*(d)+1):(i*(d))])%*%sum.zx
      # draw mu from updated distribution
      k.mu.cur[,i]=rmvnorm(1,b.star.mu%*%a.star.mu,b.star.mu)
      
      # calculate Stheta = sum[(zj - mu)(zj-mu)^T]
      for(j in 1:N.j[i]){
        diff=matrix(data=z.cur[Indices[r,j],]-k.mu.cur[,i],nrow=d,ncol=1) # dx1 vector
        sum.sigma.zx=sum.sigma.zx+diff%*%t(diff) 
      }
      # draw sigma for that cluster from IW distribution
      k.sigma.cur[,((i-1)*(d)+1):(i*(d))]=riwish(nu+N.j[i],S.cur+sum.sigma.zx)
      
      r=r+1 # complete this used cluster -> move to the next
      
      if(save==1){
        # save sampled mu and sigma for that cluster
        k.mu[((row.cur-1)*(d)+1):(row.cur*(d)),i]=k.mu.cur[,i]
        k.sigma[((row.cur-1)*(d)+1):(row.cur*(d)),((i-1)*(d)+1):(i*(d))]=k.sigma.cur[,((i-1)*(d)+1):(i*(d))]
      }
    }
  }
  
  #########################################################################
  ## step2: sample L
  
  L.cur=numeric(n)
  
  # Collect cluster mean and sigma sampled from step1 into meancov.list
  meancov.list=vector("list",N)
  for(j in 1:N){
    meancov.list[[j]]=list(k.mu.cur[,j],k.sigma.cur[,((j-1)*(d)+1):(j*(d))])
  }
  
  # Calculate p(Z|mu_l, Sigma_l) for each sample for each cluster = p(Z|L)
  norm.dens.list=lapply(meancov.list, function( item ) dmvnorm (z.cur, item[[1]], item[[2]]))
  # Turn that into a matrix of size 198x40: ij entry is p(Zi|L = j)
  norm.dens.mtrx=NULL
  for(j in 1:N){
    norm.dens.mtrx=cbind(norm.dens.mtrx,as.numeric(norm.dens.list[[j]]))
  }
  
  # Sample new cluster assignment from P(L|z) = P(z|L)P(L)
  for(i in 1:n){
    L.cur[i]=sample.int(N,size=1,prob=norm.dens.mtrx[i,]*p.probs.cur)
  }
  if(save==1){
    L[row.cur,]=L.cur
  }
  
  #update components: Number of occupied clusters
  components.cur=length(unique(L.cur))
  if(save==1)
  components[row.cur]=components.cur
  
  #########################################################################
  ## step3: Sample p.probs
  
  # Calculate Nj (Don't know why they don't use Nj)
  M.config=numeric(N)
  for(i in 1:N)
  M.config[i]=sum(L.cur==i)
  
  # Update stick breaking process: Vi
  vstar=numeric(N-1)
  for(i in 1:(N-1)){
    vstar[i]=rbeta(1,1+M.config[i],alpha.cur+sum(M.config[(i+1):N]))
    if(vstar[i]==0) vstar[i]=.00001
    if(vstar[i]==1) vstar[i]=.9999
  }
    
  # Update pi for each cluster (mixing proportion)
  # Using updated Vi and stick breaking process
  p.probs.cur[1]=vstar[1]
  prod=1
  for(i in 2:(N-1)){
    prod=prod*(1-vstar[i-1])
    p.probs.cur[i]=vstar[i]*prod
  }
  
  if((1-sum(p.probs.cur[1:(N-1)]))>=0)
  p.probs.cur[N]=1-sum(p.probs.cur[1:(N-1)]) else p.probs.cur[N]=0
  if(save==1)
  p.probs[row.cur,]=p.probs.cur
  
  #########################################################################
  # step4: sample alpha
  
  # sample new alpha (for stick breaking) from the posterior gamma dist
  alpha.cur=rgamma(1,a.alpha+N-1,rate= -sum(log(1-vstar))+b.alpha)
  if(save==1)
  alpha[row.cur]=alpha.cur
  
  #########################################################################
  # step5: sample M and V (parameter for DP NIW)
  
  # sample M from normal dist
  # Update covariance matrix
  B.m.star=solve(solve(B.m)+N*solve(V.cur))
  # Update mean vector
  sum=0
  for(i in 1:N)
  sum=sum+solve(V.cur)%*%k.mu.cur[,i]
  
  a.m.star=B.m.star%*%(solve(B.m)%*%a.m+sum)
  
  # Sample M from full conditionally
  M.cur=rmvnorm(1,a.m.star,B.m.star)
  if(save==1)
  M[row.cur,]=M.cur
  
  # sample V from IW
  sum=matrix(0,nrow=d,ncol=d)
  for(i in 1:N){
    # diff = mu-M
    mu.diff=matrix(data=k.mu.cur[,i]-M.cur,nrow=d,ncol=1)
    # calculate sum(mu-M)(mu-M)^T
    sum=sum+mu.diff%*%t(mu.diff)
  }
  V.cur=riwish(a.V+N,B.V+sum)
  
  if(save==1)
  V[((row.cur-1)*(d)+1):(row.cur*(d)),]=V.cur
  
  #########################################################################
  # step6: sample S (parameter for DP NIW)
  # The posterior of S will be Wishart
  sum.S=matrix(0,d,d)
  
  for(i in 1:N)
  sum.S=sum.S+solve(k.sigma.cur[,((i-1)*(d)+1):(i*(d))])
  
  S.cur=rwish(a.S+nu*N,solve(solve(B.S)+sum.S))
  if(save==1) S[((row.cur-1)*(d)+1):(row.cur*(d)),]=S.cur
  
  
  #########################################################################
  # step7: sample z
  # >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<
  for(i in 1:n){
    # extract current covariance matrix of that cluster
    sig=k.sigma.cur[,((L.cur[i]-1)*(d)+1):((L.cur[i])*(d))]
    # impute z1 given z2, z3 and X and other z's
    # iterate through zi
    for (z_index in 1:R) {
      # mu of zi in that cluster
      mu.zi = k.mu.cur[z_index,L.cur[i]]
      # variance/covariance of different terms
      sig.zx=matrix(data=sig[z_index,-z_index],nrow=1,ncol=d-1)
      sig.xz=t(sig.zx)
      sig.xx=matrix(data=sig[-z_index,-z_index],nrow=d-1,ncol=d-1)
      sig.zz=sig[z_index,z_index]
      # z_others - mu_other
      diff=matrix(data=z.cur[i,-z_index]-
                    k.mu.cur[-z_index,L.cur[i]],nrow=d-1,ncol=1)
      # calculate conditional mean and variance for zi
      m.z=mu.zi+sig.zx%*%solve(sig.xx)%*%diff
      s.z=sig.zz-sig.zx%*%solve(sig.xx)%*%sig.xz
      gamma_i = gamma.list[[z_index]]
      if (IndicatorMat[i,z_index]) {
        # The Y is missing
        # Sample zi from non truncated normal distribution and impute y value
        z.cur[i,z_index] = rnorm(1,mean=m.z,sd=sqrt(s.z))
        y.cur[i,z_index] = sum(z.cur[i,z_index]>gamma_i)
        # Set y according to gamma_i
      }else{
        # Sample zi from truncated normal considering yi
        z.cur[i,z_index]=rtruncnorm(1,a=gamma_i[y[i,z_index]],
                                    b=gamma_i[y[i,z_index]+1],
                                    mean=m.z,sd=sqrt(s.z))
      }

    }
  }
  # >>>>>>>>>>>>>>>> operationalized  <<<<<<<<<
  if(save==1){
    for (z_index in 1:R) {
      sampled_z[row.cur,,z_index] = z.cur[,z_index]
      sampled_y[row.cur,,z_index] = y.cur[,z_index]
    }
  }
}
```

Unpacking for comparison

```{r}
# unpack z for future analysis
z1=sampled_z[,,1]
z2=sampled_z[,,2]
z3=sampled_z[,,3]
z4=sampled_z[,,4]

y3=sampled_y[,,3]
y4=sampled_y[,,4]
```

Model checking: trace plot first sample of z3 

```{r, echo = FALSE}
# trace plot
z13.mcmc <- mcmc(z3[,1], start=1)
plot(z13.mcmc)
```

```{r, echo = FALSE}
# auto correlation
autocorr.plot(z13.mcmc)
```

```{r}
# Number of cluster in the DP process used over iterations

plot(1:length(components), components, xlab = 'iteration', 
     ylab = 'number of active clusters', 
     main = 'Number of clusters used over time', ylim = c(0,40))
```

Compare distributions

```{r}
# Original distribution without missing
y3_original = y_original[,3]
original_pmf = table(y3_original)/length(y3_original)

# Observed distribution
missing_indicator = indicatorZ3
y3_observed = y_original[!missing_indicator,3]
observed_pmf = table(y3_observed)/length(y3_observed)

# Extract variable from imputed data
y3_imputed = y3
imputed_pmf = table(y3_imputed)/length(y3_imputed)

df3 = rbind(original_pmf,observed_pmf,imputed_pmf)
colnames(df3)<- c('cat 1', 'cat 2', 'cat 3', 'cat4', 'cat5')
barplot(df3, xlab = 'Category', beside = TRUE, 
        legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Y3',
        ylim = c(0,0.5))
```


Compare distributions

```{r}
# Original distribution without missing
y4_original = y_original[,4]
original_pmf = table(y4_original)/length(y4_original)

# Observed distribution
missing_indicator = indicatorZ4
y4_observed = y_original[!missing_indicator,4]
observed_pmf = table(y4_observed)/length(y4_observed)

# Extract variable from imputed data
y4_imputed = y4
imputed_pmf = table(y4_imputed)/length(y4_imputed)

df4 = rbind(original_pmf,observed_pmf,imputed_pmf)
colnames(df4)<- c('cat 1', 'cat 2', 'cat 3', 'cat4')
barplot(df4, xlab = 'Category', beside = TRUE, 
        legend = TRUE, main = 'Blocked Gibbs Sampling Assessment: Y4',
        ylim = c(0,0.5))
```

* * *



